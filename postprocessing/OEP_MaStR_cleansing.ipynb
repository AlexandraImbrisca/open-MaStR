{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://openenergy-platform.org/static/OEP_logo_2_no_text.svg\" alt=\"OpenEnergy Platform\" height=\"100\" width=\"100\"  align=\"left\"/>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Bhl_neue_wege_logo_transparent.svg/2000px-Bhl_neue_wege_logo_transparent.svg.png\" alt=\"BHL\" height=\"300\" width=\"300\" align=\"right\"/>\n",
    "\n",
    "# OpenEnergyPlatform\n",
    "<br><br>\n",
    "\n",
    "# MaStR Data Cleansing\n",
    "Repository: https://github.com/OpenEnergyPlatform/data-preprocessing/tree/master/data-import/bnetza_mastr\n",
    "\n",
    "Please report bugs and improvements here: https://github.com/OpenEnergyPlatform/data-preprocessing/issues <br>\n",
    "How to get started with Jupyter Notebooks can be found here: https://github.com/OpenEnergyPlatform/oeplatform/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__copyright__ = \"Bauhaus Luftfahrt e.V.\"\n",
    "__license__   = \"GNU Affero General Public License Version 3 (AGPL-3.0)\"\n",
    "__url__       = \"https://github.com/openego/data_processing/blob/master/LICENSE\"\n",
    "__author__    = \"Benjamin W. Portner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#OEP-MaStR-data\" data-toc-modified-id=\"OEP-MaStR-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>OEP MaStR data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Drop-duplicates\" data-toc-modified-id=\"Drop-duplicates-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Drop duplicates</a></span></li><li><span><a href=\"#Drop-outliers\" data-toc-modified-id=\"Drop-outliers-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Drop outliers</a></span></li></ul></li><li><span><a href=\"#OPSD-data\" data-toc-modified-id=\"OPSD-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>OPSD data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-clean-data\" data-toc-modified-id=\"Load-and-clean-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load and clean data</a></span></li><li><span><a href=\"#Match-datasets\" data-toc-modified-id=\"Match-datasets-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Match datasets</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Export</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OEP MaStR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the datasets for hydro, wind and biomass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "version = '1.4'\n",
    "\n",
    "fn_wind = f'bnetza_mastr_rli_v{version}_wind'\n",
    "df_wind = pd.read_csv(f'data/OEP/bnetza_mastr_power-units_rli_v{version}/{fn_wind}.csv', \n",
    "                      encoding='utf8', sep=';', parse_dates=[\"Inbetriebnahmedatum\"], dtype={\"Postleitzahl\":str})\n",
    "\n",
    "fn_hydro = f'bnetza_mastr_rli_v{version}_hydro'\n",
    "df_hydro = pd.read_csv(f'data/OEP/bnetza_mastr_power-units_rli_v{version}/{fn_hydro}.csv', \n",
    "                       encoding='utf8', sep=';', parse_dates=[\"Inbetriebnahmedatum\"], dtype={\"Postleitzahl\":str})\n",
    "\n",
    "fn_biomass = f'bnetza_mastr_rli_v{version}_biomass'\n",
    "df_biomass = pd.read_csv(f'data/OEP/bnetza_mastr_power-units_rli_v{version}/{fn_biomass}.csv', \n",
    "                         encoding='utf8', sep=';', parse_dates=[\"Inbetriebnahmedatum\"], dtype={\"Postleitzahl\":str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And merge them into one DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_wind.append(df_hydro, ignore_index=True, sort=False).append(df_biomass, ignore_index=True, sort=False)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, there is an unnamed column. Let's drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates\n",
    "\n",
    "Next, let's see if all the entries are unique. Dropping duplicates does not work on this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df_all)\n",
    "len_after = len(df_all.drop_duplicates())\n",
    "\n",
    "len_before, len_before == len_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are quite a few entries that are basically identical: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all.duplicated(subset=\"EinheitMastrNummer\", keep=False)].sort_values(by=\"EinheitMastrNummer\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why were these not dropped in pandas's \"drop_duplicates\"? Because the dataset contains a timestamp that documents the time of download for each entry. Naturally, that timestep is unique for each entry. Let's drop it and repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one dataframe with all duplicated entries\n",
    "has_double = df_all.drop(columns=[\"timestamp_e\"]).duplicated(keep=False)\n",
    "df_duplicated = df_all[has_double].copy().sort_values(by=\"EinheitMastrNummer\").reset_index()\n",
    "\n",
    "# create one dataframe with only unique entries\n",
    "is_unique = ~df_all.drop(columns=[\"timestamp_e\"]).duplicated(keep=\"first\")\n",
    "df_unique = df_all[is_unique].copy().sort_values(by=\"EinheitMastrNummer\").reset_index()\n",
    "\n",
    "# compare sizes\n",
    "len(df_all), len(df_duplicated), len(df_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 65,133 unique entries. That is 6,004 less than in the merged dataset. There are 9,381 entries which have at least one double.\n",
    "\n",
    "Let's check if everything worked alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's export the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated.to_csv(\"output/OEP_0_duplicated.csv\", index=False)\n",
    "df_unique.to_csv(\"output/OEP_0_unique.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop outliers\n",
    "Some of the entries have coordinates which do not lie within Germany. Some of these are off-shore wind turbines. However, there are entries which have addresses in Bayern or Niedersachsen but are located in Italy, and even Africa. We want to remove these.\n",
    "\n",
    "First off, imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load some shape files. They contain the boundaries of Germany, Baltic sea, and North Sea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_DE = gpd.read_file(r\"data\\shapefiles_germany\\BKG\\vg2500_sta.shp\")\n",
    "gdf_baltic = gpd.read_file(r\"data\\shapefiles_germany\\marineregions.org\\DE_baltic_sea\\eez_iho.shp\")\n",
    "gdf_north_sea = gpd.read_file(r\"data\\shapefiles_germany\\marineregions.org\\DE_north_sea\\eez_iho.shp\")\n",
    "\n",
    "gdf_DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extend the boundaries slightly to make sure that points close to Germany's border will be correctly identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 0.01\n",
    "gdf_DE[\"geometry\"] = gdf_DE.buffer(buffer)\n",
    "gdf_baltic[\"geometry\"] = gdf_baltic.buffer(buffer)\n",
    "gdf_north_sea[\"geometry\"] = gdf_north_sea.buffer(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to test if entries lie within German territory, we first need to convert them to a GeoDataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entries without defined coordinates to prevent errors\n",
    "df_no_na = df_unique[[\"EinheitMastrNummer\",\"Laengengrad\",\"Breitengrad\"]].dropna()\n",
    "\n",
    "# make list of shapely Points\n",
    "points = [Point(xy) for xy in zip(df_no_na[\"Laengengrad\"], df_no_na[\"Breitengrad\"])]\n",
    "\n",
    "# define coordinate system WGS 84\n",
    "crs = {'init': 'epsg:4326'}\n",
    "\n",
    "# make GeoDataFrame\n",
    "gdf_points = gpd.GeoDataFrame(df_no_na[\"EinheitMastrNummer\"], crs=crs, geometry=points)\n",
    "gdf_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test for all points whether they are contained in each shape file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_in_DE = gpd.sjoin(gdf_points, gdf_DE, how=\"right\", op=\"within\")\n",
    "points_in_baltic = gpd.sjoin(gdf_points, gdf_baltic, how=\"right\", op=\"within\")\n",
    "points_in_north_sea = gpd.sjoin(gdf_points, gdf_north_sea, how=\"right\", op=\"within\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define entries as located correctly if they are:\n",
    "- wind, biomass or hydro on German soil\n",
    "- wind in German parts of Baltic Sea or North Sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "located_indices = (\n",
    "    ( df_unique[\"EinheitMastrNummer\"].isin(points_in_DE[\"EinheitMastrNummer\"]) ) |\n",
    "    (\n",
    "            ( df_unique[\"Einheittyp\"] == \"Windeinheit\" ) &\n",
    "            (\n",
    "                ( df_unique[\"EinheitMastrNummer\"].isin(points_in_baltic[\"EinheitMastrNummer\"]) ) |\n",
    "                ( df_unique[\"EinheitMastrNummer\"].isin(points_in_north_sea[\"EinheitMastrNummer\"]) )\n",
    "            )\n",
    "    )\n",
    ")\n",
    "df_located = df_unique[located_indices].copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other entries are treated as unlocated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlocated = df_unique[~located_indices].copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many entries are located / unlocated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_unique), len(df_located), len(df_unlocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51% of all unique entries have not been located or have been wrongly located. \n",
    "\n",
    "Let's export located and unlocated entries as csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_located.to_csv(\"output/OEP_1_located.csv\", index=False)\n",
    "df_unlocated.to_csv(\"output/OEP_1_unlocated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create an interactive map of located and unlocated entries. We can zoom and pan the map. When hovering over a point on the map, the entry's name, address, type, and gross output is shown as a tooltip. Also, clicking on the legend allows hiding and showing different types of powerplants.\n",
    "\n",
    "The plotting logic is contained in a separate function \"plotPowerPlants\" in the module \"helpers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "plot_located = plotPowerPlants(df_located)\n",
    "\n",
    "# export as html\n",
    "gv.renderer(\"bokeh\").save(plot_located, \"output/OEP_1_located\")\n",
    "\n",
    "# figure is very big (5 MB)! plotting in jupyter is not recommended!\n",
    "#plot_located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the unlocated entries to check if everything worked alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unlocated = plotPowerPlants(df_unlocated)\n",
    "\n",
    "# export as html\n",
    "gv.renderer(\"bokeh\").save(plot_unlocated, \"output/OEP_1_unlocated\")\n",
    "\n",
    "# show in notebook\n",
    "plot_unlocated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the outliers were identified correctly. Now, what do we do about these and about unlocated entries? Maybe we can use data from other sources to get their coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPSD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open power Systems Data (OPSD, https://open-power-system-data.org/) is another project offering power plant data for Germany (and many other European states). The datasets include powerplant type, generation capacity, and geo-coordinates (among other fields). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data\n",
    "\n",
    "There renewable plants dataset includes many different power plant types, including hydro, solar, wind, bioenergy, and geothermal. For this analysis, I will keep only those contained in the OEP dataset: hydro, bioenergy, and wind. Also, I will rename some columns for compatibility with the OEP MaStR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load renewables\n",
    "df_renewables = pd.read_csv(\n",
    "    \"data/OPSD/renewable_power_plants_DE.csv\",\n",
    "    sep=\";\", encoding=\"ANSI\", parse_dates=[\"commissioning_date\"]\n",
    ")\n",
    "\n",
    "# keep only wind, bioenergy and hydro\n",
    "df_renewables = df_renewables[df_renewables[\"energy_source_level_2\"].isin([\"Wind\",\"Bioenergy\",\"Hydro\"])]\n",
    "\n",
    "# rename columns for compatibiliy with OEP data\n",
    "df_renewables.rename(columns={\n",
    "    \"energy_source_level_2\": \"Einheittyp\",\n",
    "    \"address\" : \"Standort\",\n",
    "    \"federal_state\": \"Bundesland\",\n",
    "    \"commissioning_date\": \"Inbetriebnahmedatum\",\n",
    "    \"electrical_capacity\": \"Bruttoleistung\",\n",
    "    \"lat\": \"Breitengrad\",\n",
    "    \"lon\": \"Laengengrad\",\n",
    "    \"eeg_id\": \"AnlagenschluesselEeg\",\n",
    "}, inplace=True)\n",
    "\n",
    "# add columns which are not defined in OPSD data\n",
    "df_renewables[\"Name\"] = None\n",
    "df_renewables[\"Land\"] = \"Deutschland\"\n",
    "\n",
    "# rename types for compatibility with OEP\n",
    "df_renewables[\"Einheittyp\"].replace({\"Hydro\": \"Wasser\", \"Bioenergy\": \"Biomasse\", \"Wind\": \"Windeinheit\"}, inplace=True)\n",
    "\n",
    "df_renewables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, some biomass and hydro plants are contained only in the fossil power plant dataset. Let's load these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load conventional\n",
    "df_conv = pd.read_csv(\n",
    "    \"data/OPSD/conventional_power_plants_DE.csv\",\n",
    "    sep = \",\", encoding = \"ANSI\", decimal=\".\", dtype={\"commissioned\": str}\n",
    ")\n",
    "\n",
    "# keep only wind, bioenergy and hydro\n",
    "df_conv = df_conv[df_conv[\"energy_source_level_2\"].isin([\"Wind\",\"Bioenergy\",\"Hydro\"])]\n",
    "\n",
    "# commissioning dates are given as string with format YYYY.0 - Parse manually:\n",
    "df_conv[\"commissioned\"] = pd.to_datetime(df_conv[\"commissioned\"], format=\"%Y.0\")\n",
    "\n",
    "# rename columns for compatibiliy with OEP data\n",
    "df_conv.rename(columns={\n",
    "    \"energy_source_level_2\": \"Einheittyp\",\n",
    "    \"street\": \"Standort\",\n",
    "    \"state\": \"Bundesland\",\n",
    "    \"commissioned\": \"Inbetriebnahmedatum\",\n",
    "    \"capacity_gross_uba\": \"Bruttoleistung\",\n",
    "    \"lat\": \"Breitengrad\",\n",
    "    \"lon\": \"Laengengrad\",\n",
    "    \"eeg\": \"AnlagenschluesselEeg\",\n",
    "}, inplace=True)\n",
    "\n",
    "# add columns which are not defined in OPSD data\n",
    "df_conv[\"Land\"] = \"Deutschland\"\n",
    "df_conv[\"Name\"] = df_conv[\"name_bnetza\"].fillna(\"\") + df_conv[\"name_uba\"].fillna(\"\")\n",
    "\n",
    "# rename types for compatibility with OEP\n",
    "df_conv[\"Einheittyp\"].replace({\"Hydro\": \"Wasser\", \"Bioenergy\": \"Biomasse\", \"Wind\": \"Windeinheit\"}, inplace=True)\n",
    "\n",
    "# merge\n",
    "df_OPSD = df_renewables.append(df_conv, ignore_index=True, sort=False)\n",
    "\n",
    "df_OPSD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many entries do we have? Let's compare to the OEP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd = df_OPSD[\"Einheittyp\"].value_counts()\n",
    "oep = df_unique[\"Einheittyp\"].value_counts()\n",
    "df = pd.DataFrame(columns=opsd.index, data=[opsd.values, oep.values], index=[\"OPSD\", \"OEP\"])\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OEP MaStR dataset is more than three times larger! Still, we can extract some new coordinates from the OPSD data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPSD datasets contain columns \"eeg_id\" / \"eeg\" which are identical to OEP-MaStR dataset column \"AnlagenschluesselEeg\". I will use these columns for matching. To simplify matching, the OPSD columns have already been renamed to their OEP counter-part during loading.\n",
    "\n",
    "First, I need to remove entries without an EEG ID from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPSD_EEG = df_OPSD[~df_OPSD[\"AnlagenschluesselEeg\"].isna()]\n",
    "df_OEP_EEG = df_unique[~df_unique[\"AnlagenschluesselEeg\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the dataset sizes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd = df_OPSD_EEG[\"Einheittyp\"].value_counts()\n",
    "oep = df_OEP_EEG[\"Einheittyp\"].value_counts()\n",
    "df = pd.DataFrame(columns=opsd.index, data=[opsd.values, oep.values], index=[\"OPSD\", \"OEP\"])\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, most entries in both datasets have an EEG ID. Next, let's find the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersecting_EEGs = set(df_OPSD_EEG[\"AnlagenschluesselEeg\"]).intersection(set(df_OEP_EEG[\"AnlagenschluesselEeg\"]))\n",
    "len(intersecting_EEGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13727 entries are contained in both the OEP and in the OPSD data. This means, the OEP MaStR dataset contains 73% of the OPSD biomass, wind and hydro data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how many entries in the intersection are unlocated in the OEP dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OEP_intersected_unlocated = df_unlocated[df_unlocated[\"AnlagenschluesselEeg\"].isin(intersecting_EEGs)]\n",
    "len(df_OEP_intersected_unlocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1134 entries! Let's try to fetch their coordinates from the OPSd dataset. \n",
    "\n",
    "Note: The EEG ID is not unique, so I cannot use normal merge/join operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OPSD_intersected_located = df_OPSD_EEG[\n",
    "    ( df_OPSD_EEG[\"AnlagenschluesselEeg\"].isin(df_OEP_intersected_unlocated[\"AnlagenschluesselEeg\"]) ) &\n",
    "    ~(\n",
    "        df_OPSD_EEG[\"Laengengrad\"].isna() | \n",
    "        df_OPSD_EEG[\"Breitengrad\"].isna() \n",
    "    )\n",
    "]\n",
    "\n",
    "df_extracted = df_OEP_intersected_unlocated.copy()\n",
    "\n",
    "for OPSD_index, eeg_id in df_OPSD_intersected_located[\"AnlagenschluesselEeg\"].items():\n",
    "    OEP_indices = df_extracted[df_extracted[\"AnlagenschluesselEeg\"]==eeg_id].index\n",
    "    df_extracted.loc[OEP_indices, [\"Laengengrad\", \"Breitengrad\"]] = \\\n",
    "        df_OPSD_intersected_located.loc[OPSD_index,[\"Laengengrad\", \"Breitengrad\"]].values\n",
    "\n",
    "df_extracted.dropna(subset=[\"Laengengrad\", \"Breitengrad\"], inplace=True)\n",
    "\n",
    "len(df_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extract 1133 coordinates. All but one OPSD entry had coordinates associated with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export the dataframes as csv.\n",
    "\n",
    "Newly located entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted.to_csv(\"output/OEP_2_located_only_OPSD.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also update the list of located / unlocated entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OEP_OPSD_located = df_located.append(df_extracted, ignore_index=True)\n",
    "df_OEP_OPSD_located.to_csv(\"output/OEP_2_located_incl_OPSD.csv\", index=False)\n",
    "\n",
    "df_OEP_OPSD_unlocated = df_unique[\n",
    "    ~( df_unique[\"EinheitMastrNummer\"].isin(df_OEP_OPSD_located[\"EinheitMastrNummer\"]) )\n",
    "].copy()\n",
    "df_OEP_OPSD_unlocated.to_csv(\"output/OEP_2_unlocated_incl_OPSD.csv\", index=False)\n",
    "\n",
    "len(df_OEP_OPSD_located), len(df_OEP_OPSD_unlocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half of the entries are now located! Let's plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newly located entries (OPSD intersection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_OPSD = plotPowerPlants(df_extracted)\n",
    "gv.renderer(\"bokeh\").save(plot_OPSD, \"output/OEP_2_located_only_OPSD\")\n",
    "plot_OPSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All located entries (OPSD intersection + OEP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_located = plotPowerPlants(df_OEP_OPSD_located)\n",
    "gv.renderer(\"bokeh\").save(plot_located, \"output/OEP_2_located_incl_OPSD\")\n",
    "\n",
    "# figure is very big (5 MB)! plotting in jupyter is not recommended!\n",
    "#plot_located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unlocated = plotPowerPlants(df_OEP_OPSD_unlocated)\n",
    "gv.renderer(\"bokeh\").save(plot_unlocated, \"output/OEP_2_unlocated_incl_OPSD\")\n",
    "plot_unlocated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some outliers which could not be corrected using OPSD data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
